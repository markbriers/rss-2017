{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions and Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These notes contain instructions and questions for the labs portion of the \"Big Data: tools and statistics\" course. Within this document, command-line steps are presented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scala>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All commands will be in a separate grey \"cell\" (as above).\n",
    "<br><br>\n",
    "Exercises will be listed as a bulleted item and italicized. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li><i>Create a new directory in your home directory called sample. Upload data.csv into the sample directory on HDFS.</i></li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will be expected to achieve the following:\n",
    "<ol>\n",
    "<li>Create a Spark Resilient Distributed Dataset (RDD)\n",
    "<li>Transform an RDD\n",
    "<li>Perform an action on an RDD\n",
    "<li>Perform Map Reduce operations in Spark\n",
    "<li>Understand and use mllib for statistical analysis of data\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to the Spark cluster from a windows PC, follow these instructions:\n",
    "<ol>\n",
    "<li>Start Putty (type putty into the search bar on the start menu).</li>\n",
    "<li>In the hostname textbox, type the HOSTNAME field provided on the printout, ensuring that Connection type is set to ssh.</li>\n",
    "<li>Click Open.</li>\n",
    "<li>On first connection, you will be asked a question about connection security. Please click \"yes\".\n",
    "<li>You will be asked for your username and password.</li>\n",
    "<li>Usernames are trainingN (with N replaced by your allocated number) and password, provided on the printout.</li>\n",
    "</ol>\n",
    "\n",
    "To connect to the Spark cluster from a Mac, follow these instructions:\n",
    "<ol>\n",
    "<li>Start Terminal (type terminal into the spotlight search bar).</li>\n",
    "<li>Type the following command, followed by enter:\n",
    "<br>ssh HOSTNAME </li>\n",
    "<li>On first connection, you will be asked a question about connection security. Please type \"yes\".\n",
    "<li>You will be asked for your username and password.</li>\n",
    "<li>Usernames are trainingN (with N replaced by your allocated number) and password, provided on the printout.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now connect to the Spark REPL, using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark-shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a short delay, you will be presented with a new prompt that looks like follows (NB: it is safe to ignore any error messages when Spark loads):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid ambiguity, all Spark commands listed within this document will follow this <i>scala</i> prompt.<br>\n",
    "As part of the Spark startup process, a Spark context (sc) is created, which is the main interface to the requested Spark environment. To view the available functions, type the following and then immediately press the Tab key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> sc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now create an RDD by loading the Heathrow airport temperature data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val heathrowFile = sc.textFile(\"hdfs:///user/USERNAME/temperatureData/heathrowdata.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that <b>USERNAME</b> will need to be replaced with your compute cluster username, <i>userN</i>.\n",
    "<br><br>\n",
    "As displayed in the console, the value heathrowFile is of type org.apache.spark.rdd.RDD[String], that is, the function textFile has created an RDD of type String (as one would perhaps expect when reading a file of text). Due to Scala’s lazy evaluation model, the file has not been read; the file has not even been checked to see if it exists! This only happens when an action is performed. You will now bring back 5 lines of data to the console and print each line, to force Spark to undertake an <i>action</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val fiveLines = heathrowFile.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> fiveLines.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line creates a value fiveLines of the RDD by taking the first 5 elements (lines in the case of a textFile). fiveLines is of type Array[String] as the memory is now being held and managed in the local console, rather than via Spark’s RDD. The second line calls the function println on each element of fiveLines, which subsequently displays the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Create a value called wickairportFile, that is an RDD[String], based on the wickairportdata.txt file in HDFS. Display the first five lines of this data file.</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now create a function that is able to determine whether a line of data contains the header information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> def isHeader(line: String): Boolean = {\n",
    "line.contains(\"yyyy\") || line.split(\" \").size != 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function is created by using the keyword def, followed by the function name, input arguments, and output type. The function body is specified in between the braces. This function simply checks to see whether the string yyyy is present in the line of data that is passed into the function. The string yyyy is only present in the header line.\n",
    "<br><br>\n",
    "You will now create a Case class corresponding to the structure of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> case class TemperatureRecord(year: Long, month: Int, tmax: Float, tmin: Float, rain: Float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class will act as a data structure to allow us to parse and manipulate the data in a convenient manner. You can create an RDD of TemperatureRecord objects by creating a parse function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> def parse(line: String) = {\n",
    "val pieces = line.split('\\t')\n",
    "val year = pieces(0).toLong\n",
    "val mm = pieces(1).toInt\n",
    "val tmax = pieces(2).toFloat\n",
    "val tmin = pieces(3).toFloat\n",
    "val rain = pieces(4).toFloat\n",
    "TemperatureRecord(year, mm, tmax, tmin, rain)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and by creating the following transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val heathrowData = heathrowFile.filter(x => !isHeader(x)).map(parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, due to Spark’s lazy evaluation model, this command will not be executed until we perform an action on the data. The value heathrowData is of type org.apache.spark.rdd.RDD[TemperatureRecord]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Perform the same transformation on the value wickairportFile, to create a value wickairportData, which is of type org.apache.spark.rdd.RDD[TemperatureRecord]. NB: You will need to create an additional filter transformation to filter the lines of data in the wickairportFile that contain missing values (i.e. the records that contain “---“ as default data files). [It is recommended that you test this set of transformations using the take command.]</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often the case that you need to sort the data in accordance with one of the fields in the data. You will now sort the data by month rather than year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala > val heathrowMonth = heathrowData.sortBy(_.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Sort the wickairportData by maximum temperature, creating a value called wickairportTemp. <b>NB: As often occurs in data analysis, you will need to clean the data first, by filtering any erroneous data points that contain \"---\" as the temperature values.</b><i>\n",
    "</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now convert all temperature values from degrees Celsius into degrees Fahrenheit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val heathrowMonthFah = heathrowMonth.map( x => {\n",
    "val tmax = x.tmax * 9/5 + 32\n",
    "val tmin = x.tmin * 9/5 + 32\n",
    "TemperatureRecord(x.year, x.month, tmax, tmin, x.rain)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Create an RDD[Float] containing the monthly rainfall in centimeters for Wick airport (the rainfall data is represented in mm in the data), sorted by tmax.</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Create two RDDs (heathrowData and wickairportData) of type org.apache.spark.rdd.RDD[TemperatureRecord]. NB: Remember to remove header lines and missing data lines.\n",
    "<li>Using the function that you have created to remove missing data (or otherwise), make a note of the year(s) and month(s) of the missing data records.</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to replicate Map Reduce processing in Spark. Consider the following statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val heathrowAverageRain = heathrowData.map(x => (x.year,x.rain)).aggregateByKey((0.0, 0.0))((acc, value) => (acc._1 + value, acc._2 + 1), (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)).mapValues(sumCount => 1.0 * sumCount._1 / sumCount._2).sortBy(_._1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD heathrowAverageRain is of type org.apache.spark.rdd.RDD[(Long, Double)], with the first element representing the year and the second element representing the average rainfall in that year. The RDD is sorted by year, as shown by the final function call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three key components used in this transformation.\n",
    "<ol>\n",
    "<li>The map function extracts the (year, rainfall) (key,value) pairs. The output type of this transformation is org.apache.spark.rdd.RDD[(Long, Float)], with an entry per line in the data.\n",
    "<li>The aggregateByKey function computes a pair of two values for each key (year); the first is the sum of rainfall, and the second is a count of the number of elements. These counts are both initialized with 0.0. The output type of the second transformation is org.apache.spark.rdd.RDD[(Long, (Double, Double))], linking the key (year) to the two aforementioned aggregated values.\n",
    "<li>The third function, mapValues, computes the average rainfall for each key by combining the two Double values.\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Using a similar transformation to that above, compute the average monthly max temperature for both airports (heathrowAverageTMax and wickAverageTMax).</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Using the appropriate information contained on the following webpage: http://spark.apache.org/docs/latest/programming-guide.html#transformations, join the heathrowData and wickairportData datasets (using the join operation) to create an RDD called combinedData. The output should be of type org.apache.spark.rdd.RDD[((Int, Long), (Float, Float))], where the tuple corresponds to the (year, month) and the second tuple corresponds to the (Heathrow.TMax, Wick.TMax).</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to use mllib to compute basic summary statistics of the data using the following exemplar commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> import org.apache.spark.mllib.linalg.Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how to convert the heathrowData.rain field into a RDD[Vector]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val observations = heathrowData.map(_.rain).map(x => Vectors.dense(x.toDouble))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the constructor for a dense vector takes an array of double values - it may be necessary to convert each Tuple record to an array of doubles to produce the required RSS[Vector]; see <a href=\"https://spark.apache.org/docs/1.5.1/api/java/org/apache/spark/mllib/linalg/Vectors.html\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val summary: MultivariateStatisticalSummary = Statistics.colStats(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> println(summary.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> println(summary.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that <i>observations</i> is an RDD[Vector], which can be constructed by converting the input array into a dense Vector (see http://spark.apache.org/docs/latest/mllib-data-types.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Compute summary statistics for each monthly max temperature data (summary statistics for each airport), using the appropriate columns of the joined RDD (combinedData) from the previous exercise.</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command produces the Pearson correlation coefficient for two data series (labelled seriesX and seriesY here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> import org.apache.spark.mllib.stat.Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val correlation = Statistics.corr(seriesX, seriesY, \"pearson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Compute the Pearson correlation coefficient for the two average max temperature datasets computed in Exercise 2 (heathrowAverageTMax and wickAverageTMax). What does this tell you about the data?</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands demonstrate how to estimate the parameters of a linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> import org.apache.spark.mllib.regression.LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> import org.apache.spark.mllib.regression.LinearRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> import org.apache.spark.mllib.regression.LinearRegressionWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> import org.apache.spark.mllib.linalg.Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val data = sc.textFile(\"data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val parsedData = data.map { line =>\n",
    "  val parts = line.split(',')\n",
    "  LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))\n",
    "}.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val numIterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val model = LinearRegressionWithSGD.train(parsedData, numIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on training examples and compute training error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val valuesAndPreds = parsedData.map { point =>\n",
    "  val prediction = model.predict(point.features)\n",
    "  (point.label, prediction)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val MSE = valuesAndPreds.map{case(v, p) => math.pow((v - p), 2)}.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> println(\"training Mean Squared Error = \" + MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Estimate the parameters of a linear regression model using the combined data, with max temperature for Heathrow airport as the input and max temperature for Wick as the output variable.</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HINT: The default step size is too large for this particular example. It is possible to reduce the step size to a smaller value (0.01 is recommended). The train function, and its input parameters, are described <a href=\"https://spark.apache.org/docs/1.5.2/api/java/org/apache/spark/mllib/regression/LinearRegressionWithSGD.html#train(org.apache.spark.rdd.RDD,%20int,%20double)\"> here</a>."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
