{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 Instructions and Exercises (Big Data in Statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These notes contain instructions and questions for the labs portion of the Big Data in Statistics module. Within this document, command-line steps are presented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -put data /user/mark/repository/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All commands will be in a separate grey \"cell\" (as above).\n",
    "<br><br>\n",
    "Exercises will be listed as a bulleted item and italicized. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li><i>Create a new directory in your HDFS home directory called sample. Upload data.csv into the sample directory on HDFS.</i></li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To follow real-world development practices, you will be using configuration control software git, and internet based repositories on <a href=\"http://github.com\">github.com</a>. Instructions will be provided on how to use these tools during the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will be expected to achieve the following:\n",
    "<ol>\n",
    "<li>Perform Map Reduce operations in Spark\n",
    "<li>Understand and use mllib for statistical analysis of data\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the Spark REPL.\n",
    "<ul>\n",
    "<li><i>Following the previous set of Spark exercises (W3), create two RDDs (heathrowData and wickairportData) of type org.apache.spark.rdd.RDD[TemperatureRecord]. NB: Remember to remove header lines and missing data lines.\n",
    "<li>Using the function that you have created to remove missing data (or otherwise), make a note of the year(s) and month(s) of the missing data records.</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to replicate Map Reduce processing in Spark. Consider the following statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val heathrowAverageRain = heathrowData.map(x => (x.year,x.rain)).aggregateByKey((0.0, 0.0))((acc, value) => (acc._1 + value, acc._2 + 1), (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)).mapValues(sumCount => 1.0 * sumCount._1 / sumCount._2).sortBy(_._1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD heathrowAverageRain is of type org.apache.spark.rdd.RDD[(Long, Double)], with the first element representing the year and the second element representing the average rainfall in that year. The RDD is sorted by year, as shown by the final function call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three key components used in this transformation.\n",
    "<ol>\n",
    "<li>The map function extracts the (year, rainfall) (key,value) pairs. The output type of this transformation is org.apache.spark.rdd.RDD[(Long, Float)], with an entry per line in the data.\n",
    "<li>The aggregateByKey function computes a pair of two values for each key (year); the first is the sum of rainfall, and the second is a count of the number of elements. These counts are both initialized with 0.0. The output type of the second transformation is org.apache.spark.rdd.RDD[(Long, (Double, Double))], linking the key (year) to the two aforementioned aggregated values.\n",
    "<li>The third function, mapValues, computes the average rainfall for each key by combining the two Double values.\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Using a similar transformation to that above, compute the average monthly max temperature for both airports (heathrowAverageTMax and wickAverageTMax).</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Using the appropriate information contained on the following webpage: http://spark.apache.org/docs/latest/programming-guide.html#transformations, join the heathrowData and wickairportData datasets (using the join operation) to create an RDD called combinedData. The output should be of type org.apache.spark.rdd.RDD[((Int, Long), (Float, Float))], where the tuple corresponds to the (year, month) and the second tuple corresponds to the (Heathrow.TMax, Wick.TMax).</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to use mllib to compute basic summary statistics of the data using the following exemplar commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> import org.apache.spark.mllib.linalg.Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how to convert the heathrowData.rain field into a RDD[Vector]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val observations = heathrowData.map(_.rain).map(x => Vectors.dense(x.toDouble))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the constructor for a dense vector takes an array of double values - it may be necessary to convert each Tuple record to an array of doubles to produce the required RSS[Vector]; see <a href=\"https://spark.apache.org/docs/1.5.1/api/java/org/apache/spark/mllib/linalg/Vectors.html\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val summary: MultivariateStatisticalSummary = Statistics.colStats(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> println(summary.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> println(summary.variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that <i>observations</i> is an RDD[Vector], which can be constructed by converting the input array into a dense Vector (see http://spark.apache.org/docs/latest/mllib-data-types.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Compute summary statistics for each monthly max temperature data (summary statistics for each airport), using the appropriate columns of the joined RDD (combinedData) from the previous exercise.</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command produces the Pearson correlation coefficient for two data series (labelled seriesX and seriesY here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> import org.apache.spark.mllib.stat.Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val correlation = Statistics.corr(seriesX, seriesY, \"pearson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Compute the Pearson correlation coefficient for the two average max temperature datasets computed in Exercise 2 (heathrowAverageTMax and wickAverageTMax). What does this tell you about the data?</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands demonstrate how to estimate the parameters of a linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> import org.apache.spark.mllib.regression.LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> import org.apache.spark.mllib.regression.LinearRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> import org.apache.spark.mllib.regression.LinearRegressionWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> import org.apache.spark.mllib.linalg.Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val data = sc.textFile(\"data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val parsedData = data.map { line =>\n",
    "  val parts = line.split(',')\n",
    "  LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))\n",
    "}.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val numIterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val model = LinearRegressionWithSGD.train(parsedData, numIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on training examples and compute training error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val valuesAndPreds = parsedData.map { point =>\n",
    "  val prediction = model.predict(point.features)\n",
    "  (point.label, prediction)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val MSE = valuesAndPreds.map{case(v, p) => math.pow((v - p), 2)}.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> println(\"training Mean Squared Error = \" + MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Estimate the parameters of a linear regression model using the combined data, with max temperature for Heathrow airport as the input and max temperature for Wick as the output variable.</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HINT: The default step size is too large for this particular example. It is possible to reduce the step size to a smaller value (0.01 is recommended). The train function, and its input parameters, are described <a href=\"https://spark.apache.org/docs/1.5.2/api/java/org/apache/spark/mllib/regression/LinearRegressionWithSGD.html#train(org.apache.spark.rdd.RDD,%20int,%20double)\"> here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Using the trained model, predict the values of the max temperature data for Wick airport using the max temperature value for the Heathrow airport at the year and month of the missing Wick data points (using the output from Exercise 1b).</i>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
